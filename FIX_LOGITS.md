# CORRE√á√ÉO CR√çTICA - BCEWithLogitsLoss

## üö® Problema Identificado

**Treinamento anterior falhou completamente:**
- Train Loss: 1.80 (deveria ser ~0.30-0.40)
- Val AUC: 0.53 (random guess = 0.50)
- Val F1: 0.9231 (travado - sempre prevendo FAKE)
- Modelo n√£o aprendeu nada!

### Causa Raiz:
```python
# ‚ùå ERRADO:
model.forward() ‚Üí retorna sigmoid(logits)  # probabilidades [0,1]
criterion = BCEWithLogitsLoss()            # espera logits [-‚àû,+‚àû]

# Resultado: loss explode porque sigmoid(probabilidades) n√£o s√£o logits!
```

---

## ‚úÖ Corre√ß√µes Implementadas

### 1. Modelo Retorna Logits Agora

**Arquivo:** `src/model.py`

```python
class DeepfakeDetector(nn.Module):
    def __init__(self, ..., return_logits=True):
        self.return_logits = return_logits
    
    def forward(self, x):
        # ...
        logits = self.fc(x)  # (batch_size, 1) - raw scores
        
        if self.return_logits:
            return logits  # ‚úÖ Para BCEWithLogitsLoss
        else:
            return self.sigmoid(logits)  # Para inference
```

**Mudan√ßa:**
- ‚úÖ `return_logits=True` (padr√£o): retorna logits para treinamento
- ‚úÖ `return_logits=False`: retorna probabilidades para inference/avalia√ß√£o

### 2. pos_weight Corrigido

**Arquivo:** `src/train.py`

```python
# ‚ùå ANTES (errado):
pos_weight = weight_real / weight_fake  # = 6.0 (muito alto!)

# ‚úÖ AGORA (correto):
pos_weight = num_real / num_fake  # = 700/4200 = 0.167
```

**Por qu√™?**
- `pos_weight` em `BCEWithLogitsLoss` penaliza positivos (FAKE=1)
- Como queremos balancear REAL (minoria), usamos pos_weight < 1
- Isso faz o modelo dar mais import√¢ncia para acertar REAL

### 3. create_model Atualizado

**Arquivo:** `src/train.py`

```python
# ‚úÖ NOVO:
model = create_model(
    num_frames=num_frames, 
    pretrained=True, 
    device=device, 
    return_logits=True  # ‚Üê Adicionado
)
```

---

## üìä Resultados Esperados Agora

### Loss:
```
√âpoca | Train Loss | Val Loss | Esperado
------|------------|----------|----------
  1   | ~0.60-0.70 | ~0.55-0.65| ‚úÖ Decrescendo
  5   | ~0.35-0.45 | ~0.38-0.48| ‚úÖ Convergindo
 10   | ~0.25-0.35 | ~0.30-0.40| ‚úÖ Est√°vel
```

### M√©tricas:
```
Val AUC: 0.53 ‚Üí 0.70-0.85  ‚úÖ Melhorando
Val F1:  0.92 ‚Üí 0.75-0.85  ‚úÖ Balanceado (ambas classes)
```

### Comportamento:
- ‚úÖ Train loss diminui consistentemente
- ‚úÖ Val loss segue train loss
- ‚úÖ AUC aumenta progressivamente
- ‚úÖ F1 n√£o fica travado (prev√™ ambas classes)

---

## üîß O Que Foi Mudado

| Arquivo | Mudan√ßa | Linha(s) |
|---------|---------|----------|
| `src/model.py` | Adicionar `return_logits` param | 18, 21 |
| `src/model.py` | Modificar `forward()` para retornar logits | 107-113 |
| `src/model.py` | Adicionar `return_logits` em `create_model()` | 166, 182 |
| `src/train.py` | Passar `return_logits=True` em `create_model()` | 220 |
| `src/train.py` | Corrigir c√°lculo `pos_weight` | 233-241 |

---

## üéØ Teste R√°pido (5 min)

Para verificar se funcionou, rode **1 √©poca apenas**:

```python
# No train_full.py, temporariamente:
config = {
    'num_epochs': 1,  # ‚Üê Teste
    # ...
}
```

**Checklist de Sucesso:**
- [ ] Train Loss < 0.70 na primeira √©poca
- [ ] Val Loss < 0.65 na primeira √©poca  
- [ ] Val AUC > 0.55 (melhor que random)
- [ ] Loss diminui ao longo dos batches

Se tudo OK, rodar treinamento completo (10-20 √©pocas).

---

## üöÄ Comando para Treinar

```cmd
.venv-1\Scripts\python.exe train_full.py
```

**Tempo estimado:** 10-15 horas (8 √©pocas √ó 1-2h)

---

## üìù Compatibilidade com C√≥digo Antigo

**Interface/Inference:** Precisa usar `return_logits=False`

```python
# Para inference:
model = create_model(..., return_logits=False)
# OU
model.return_logits = False

# Agora model(x) retorna probabilidades [0,1]
```

**Modelos Salvos:** Modelos antigos funcionam! O par√¢metro `return_logits` √© adicionado com padr√£o `True`.

---

**Status:** ‚úÖ CORRE√á√ïES CR√çTICAS APLICADAS  
**Data:** 30 de outubro de 2025  
**Pronto para:** Novo treinamento
