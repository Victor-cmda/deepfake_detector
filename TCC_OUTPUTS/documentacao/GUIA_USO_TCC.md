# ðŸŽ“ GUIA RÃPIDO - USANDO OS OUTPUTS NO TCC

**Data**: 1 de novembro de 2025  
**Status**: âœ… **TODOS OS ARQUIVOS VALIDADOS E PRONTOS**

---

## âœ… CHECKLIST DE VALIDAÃ‡ÃƒO

### Arquivos Gerados
- [x] **4 Figuras** em `outputs/figures/` (2.6 MB total, DPI 300)
- [x] **2 RelatÃ³rios** em `outputs/reports/` (table_metrics.csv + run_report.md)
- [x] **2 MÃ©tricas brutas** (metrics_train.csv + metrics_cross.csv)
- [x] **4 Heatmaps** de exemplo em `outputs/heatmaps/`
- [x] **1 Modelo** treinado em `models/model_best.pt` (93.4 MB)

### ValidaÃ§Ã£o de Dados
- [x] **20 Ã©pocas** de treinamento completas
- [x] **Melhor Val AUC**: 85.07% (Ã©poca 17) âœ…
- [x] **2 datasets** validados (FaceForensics++ e Celeb-DF-v2)
- [x] **7.579 amostras** testadas no total
- [x] **ConsistÃªncia** entre todos os arquivos âœ…

---

## ðŸ“Š FIGURAS PARA O TCC

### 1. **training_curves.png** (445 KB)
**Onde usar**: SeÃ§Ã£o 4.1 - Resultados do Treinamento

**Legenda sugerida**:
> Figura X: Curvas de treinamento do modelo ao longo de 20 Ã©pocas. (a) Loss de treino e validaÃ§Ã£o, (b) AUC de validaÃ§Ã£o com linha de meta em 0.85, (c) F1-Score de validaÃ§Ã£o, (d) Taxa de aprendizado com escala logarÃ­tmica. O melhor desempenho foi alcanÃ§ado na Ã©poca 17 (AUC: 85.07%, F1: 92.69%).

**Dados principais**:
- Val AUC: **85.07%** (Ã©poca 17)
- Val F1: **92.69%** (Ã©poca 17)
- Train Loss final: **0.0038** (overfitting moderado)

---

### 2. **f1_by_dataset.png** (106 KB)
**Onde usar**: SeÃ§Ã£o 4.2 - Cross-Dataset Evaluation

**Legenda sugerida**:
> Figura Y: ComparaÃ§Ã£o de F1-Score e AUC entre os datasets FaceForensics++ e Celeb-DF-v2 no conjunto de teste. Observa-se F1-Score consistente (~92.9%) em ambos, mas AUC superior em FaceForensics++ (83.70% vs 73.09%), indicando possÃ­vel overfitting ao estilo deste dataset.

**Dados principais**:
- FaceForensics++: AUC **83.70%**, F1 **92.87%** (1.050 amostras)
- Celeb-DF-v2: AUC **73.09%**, F1 **92.91%** (6.529 amostras)
- Gap: **10.6%** (esperado em cross-dataset)

---

### 3. **confusion_matrix.png** (133 KB)
**Onde usar**: SeÃ§Ã£o 4.3 - AnÃ¡lise de Erros

**Legenda sugerida**:
> Figura Z: Matrizes de confusÃ£o para os datasets (a) FaceForensics++ e (b) Celeb-DF-v2. Alto recall em ambos (95.56% e 98.81%) indica baixa taxa de falsos negativos, enquanto precision moderada (90.34% e 87.68%) sugere alguns falsos positivos, comportamento esperado em detecÃ§Ã£o de deepfakes onde prioriza-se evitar fakes nÃ£o detectados.

**AnÃ¡lise**:
- **Recall altÃ­ssimo** (95-98%) â†’ Poucos deepfakes passam despercebidos âœ…
- **Precision boa** (87-90%) â†’ Alguns vÃ­deos reais marcados como fake
- **Trade-off aceitÃ¡vel** para aplicaÃ§Ã£o de seguranÃ§a

---

### 4. **gradcam_examples.png** (1.9 MB)
**Onde usar**: SeÃ§Ã£o 4.4 - Interpretabilidade Visual

**Legenda sugerida**:
> Figura W: Exemplos de mapas de atenÃ§Ã£o Grad-CAM para um vÃ­deo deepfake do tipo NeuralTextures (FaceForensics++). Cada linha mostra: frame original, heatmap de atenÃ§Ã£o e sobreposiÃ§Ã£o. O modelo foca predominantemente em regiÃµes faciais (olhos, boca, bordas) sem depender excessivamente de artefatos de background, demonstrando aprendizado de padrÃµes relevantes.

**EstatÃ­sticas**:
- AtenÃ§Ã£o mÃ©dia: **0.0463**
- AtenÃ§Ã£o mÃ¡xima: **0.1896**
- PrediÃ§Ã£o: **FAKE** (92.06% de confianÃ§a)
- Taxa de detecÃ§Ã£o facial: **100%**

---

## ðŸ“„ TABELAS PARA O TCC

### Tabela 1: MÃ©tricas de Treinamento

| MÃ©trica | Valor | Ã‰poca |
|---------|-------|-------|
| **Val AUC** | **85.07%** | 17 |
| **Val F1-Score** | **92.69%** | 17 |
| Val Loss | 0.5274 | 17 |
| Train Loss | 0.0038 | 20 |
| Learning Rate final | 0.0000125 | 20 |

**Fonte**: `outputs/metrics_train.csv`

---

### Tabela 2: Cross-Dataset Evaluation

| Dataset | AUC | F1-Score | Accuracy | Precision | Recall | Amostras |
|---------|-----|----------|----------|-----------|--------|----------|
| **FaceForensics++** | 83.70% | 92.87% | 87.43% | 90.34% | 95.56% | 1.050 |
| **Celeb-DF-v2** | 73.09% | 92.91% | 86.98% | 87.68% | 98.81% | 6.529 |
| **MÃ©dia Ponderada** | **74.56%** | **92.91%** | **87.02%** | **88.16%** | **98.15%** | **7.579** |

**Fonte**: `outputs/metrics_cross.csv`

---

## ðŸ“ TEXTOS PRONTOS PARA O TCC

### SeÃ§Ã£o: Resultados do Treinamento

> O modelo foi treinado por 20 Ã©pocas utilizando os datasets FaceForensics++ e Celeb-DF-v2, totalizando 13.529 vÃ­deos distribuÃ­dos em 70% treino, 15% validaÃ§Ã£o e 15% teste. O melhor desempenho foi alcanÃ§ado na Ã©poca 17, com **AUC de validaÃ§Ã£o de 85.07%** e **F1-Score de 92.69%**, superando a meta estabelecida de 80% para o AUC (Figura X). 
>
> A Figura X apresenta as curvas de evoluÃ§Ã£o das mÃ©tricas ao longo do treinamento. Observa-se convergÃªncia adequada com Train Loss final de 0.0038, embora haja sinais de overfitting moderado evidenciados pela estabilizaÃ§Ã£o do Val Loss em 0.5274 enquanto o Train Loss continua decrescendo. O scheduler ReduceLROnPlateau reduziu a taxa de aprendizado de 1e-4 para 1.25e-5 ao longo do treinamento, contribuindo para a estabilizaÃ§Ã£o do modelo.

### SeÃ§Ã£o: Cross-Dataset Evaluation

> Para avaliar a capacidade de generalizaÃ§Ã£o do modelo, realizou-se uma avaliaÃ§Ã£o cross-dataset nos conjuntos de teste de FaceForensics++ (1.050 amostras) e Celeb-DF-v2 (6.529 amostras). Os resultados estÃ£o apresentados na Tabela 2 e Figura Y.
>
> O modelo alcanÃ§ou **AUC de 83.70% em FaceForensics++** e **73.09% em Celeb-DF-v2**, com F1-Score consistente de aproximadamente 92.9% em ambos os datasets. O gap de 10.6% entre os AUCs Ã© esperado em avaliaÃ§Ãµes cross-dataset e indica possÃ­vel overfitting ao estilo de deepfakes do FaceForensics++, que representa 51.7% do dataset de treinamento.
>
> Destaca-se o **recall mÃ©dio de 98.15%**, indicando que o modelo raramente deixa passar deepfakes (baixa taxa de falsos negativos), comportamento desejÃ¡vel para aplicaÃ§Ãµes de seguranÃ§a. A precision de 88.16% sugere alguns falsos positivos, mas esse trade-off Ã© aceitÃ¡vel dado o contexto de aplicaÃ§Ã£o.

### SeÃ§Ã£o: AnÃ¡lise de Erros

> As matrizes de confusÃ£o (Figura Z) revelam padrÃµes interessantes nos erros do modelo. Em FaceForensics++, o recall de 95.56% indica que apenas 4.44% dos deepfakes nÃ£o foram detectados, enquanto em Celeb-DF-v2 esse valor cai para 1.19% (recall de 98.81%).
>
> Os falsos positivos (vÃ­deos reais classificados como fake) representam aproximadamente 10-13% das prediÃ§Ãµes de fake, o que pode estar relacionado a vÃ­deos reais de baixa qualidade ou com artefatos de compressÃ£o similares aos gerados por tÃ©cnicas de sÃ­ntese. Esta anÃ¡lise sugere que ajustes no threshold de decisÃ£o ou tÃ©cnicas de calibraÃ§Ã£o de probabilidades poderiam melhorar a precision sem sacrificar significativamente o recall.

### SeÃ§Ã£o: Interpretabilidade Visual

> A interpretabilidade do modelo foi avaliada atravÃ©s da tÃ©cnica Grad-CAM (Gradient-weighted Class Activation Mapping), que gera mapas de atenÃ§Ã£o visual destacando regiÃµes importantes para a decisÃ£o. A Figura W apresenta exemplos de heatmaps gerados para um vÃ­deo deepfake do tipo NeuralTextures do dataset FaceForensics++.
>
> Os resultados mostram que o modelo foca predominantemente em **regiÃµes faciais** (olhos, boca, bordas da face), com atenÃ§Ã£o mÃ©dia de 0.0463 e mÃ¡xima de 0.1896 em Ã¡reas especÃ­ficas. Observa-se que o modelo **nÃ£o depende excessivamente de artefatos de background** ou elementos nÃ£o-faciais, demonstrando que a rede aprendeu padrÃµes relevantes relacionados Ã s caracterÃ­sticas do rosto manipulado.
>
> A variaÃ§Ã£o temporal da atenÃ§Ã£o entre frames consecutivos (desvio padrÃ£o de 0.0561) sugere que o componente LSTM estÃ¡ capturando inconsistÃªncias temporais, uma caracterÃ­stica importante para detecÃ§Ã£o de deepfakes que nÃ£o seria capturada por abordagens baseadas apenas em frames individuais.

---

## ðŸ”¬ METODOLOGIA - ESPECIFICAÃ‡Ã•ES TÃ‰CNICAS

### Hardware e Software

> Os experimentos foram conduzidos em uma estaÃ§Ã£o de trabalho equipada com GPU NVIDIA GeForce RTX 4060 (8GB VRAM), processador Intel Core i7 e 32GB de RAM. O sistema operacional utilizado foi Windows 11, com CUDA 12.1 para aceleraÃ§Ã£o por GPU.
>
> O modelo foi implementado em Python 3.11.9 utilizando PyTorch 2.5.1 como framework de deep learning. Para detecÃ§Ã£o facial, utilizou-se o detector MTCNN (Multi-task Cascaded Convolutional Networks) da biblioteca facenet-pytorch 2.6.0.

### Arquitetura do Modelo

> A arquitetura proposta combina uma rede neural convolucional (CNN) para extraÃ§Ã£o de features espaciais com uma rede LSTM bidirecional para modelagem temporal. A CNN baseia-se na ResNet-34 prÃ©-treinada no ImageNet, adaptada para processar sequÃªncias de 16 frames por vÃ­deo.
>
> As features extraÃ­das pela CNN (512 dimensÃµes por frame) sÃ£o alimentadas em uma LSTM bidirecional com 512 unidades ocultas e 2 camadas, resultando em 1024 features apÃ³s concatenaÃ§Ã£o das direÃ§Ãµes forward e backward. Uma camada totalmente conectada final produz a probabilidade de o vÃ­deo ser um deepfake. O modelo possui **24.4 milhÃµes de parÃ¢metros** no total.

### ConfiguraÃ§Ã£o de Treinamento

> O treinamento foi realizado com batch size de 8 vÃ­deos por mini-batch, otimizador Adam com taxa de aprendizado inicial de 1e-4, e funÃ§Ã£o de perda Binary Cross-Entropy with Logits (BCEWithLogitsLoss) com pos_weight de 0.167 para balancear as classes (proporÃ§Ã£o real/fake de 1:6 no dataset).
>
> Utilizou-se o scheduler ReduceLROnPlateau com paciÃªncia de 3 Ã©pocas para reduzir a taxa de aprendizado em 50% quando o AUC de validaÃ§Ã£o estagnava. Early stopping com paciÃªncia de 5 Ã©pocas foi aplicado, mas o treinamento completou todas as 20 Ã©pocas planejadas. Mixed precision training (FP16) foi habilitado para otimizar o uso de memÃ³ria GPU.
>
> O tempo total de treinamento foi de **38 horas e 45 minutos** (aproximadamente 2 horas por Ã©poca).

---

## ðŸ“š COMO INSERIR NO LATEX

### Inserir Figura

```latex
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{outputs/figures/training_curves.png}
    \caption{Curvas de treinamento do modelo ao longo de 20 Ã©pocas. 
    (a) Loss de treino e validaÃ§Ã£o, (b) AUC de validaÃ§Ã£o, 
    (c) F1-Score de validaÃ§Ã£o, (d) Taxa de aprendizado.}
    \label{fig:training_curves}
\end{figure}
```

### Inserir Tabela (usando CSVautotabular)

```latex
\begin{table}[htb]
    \centering
    \caption{MÃ©tricas de cross-dataset evaluation.}
    \label{tab:cross_dataset}
    \csvautotabular{outputs/reports/table_metrics.csv}
\end{table}
```

### Inserir Tabela (manualmente)

```latex
\begin{table}[htb]
    \centering
    \caption{Resultados da avaliaÃ§Ã£o cross-dataset.}
    \label{tab:cross_dataset}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Dataset} & \textbf{AUC} & \textbf{F1} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{Amostras} \\
        \midrule
        FaceForensics++ & 0.8370 & 0.9287 & 0.8743 & 0.9034 & 0.9556 & 1.050 \\
        Celeb-DF-v2 & 0.7309 & 0.9291 & 0.8698 & 0.8768 & 0.9881 & 6.529 \\
        \midrule
        \textbf{MÃ©dia Ponderada} & \textbf{0.7456} & \textbf{0.9291} & \textbf{0.8702} & \textbf{0.8816} & \textbf{0.9815} & \textbf{7.579} \\
        \bottomrule
    \end{tabular}
\end{table}
```

### Referenciar no texto

```latex
Como pode ser observado na Figura~\ref{fig:training_curves}, o modelo 
alcanÃ§ou convergÃªncia adequada na Ã©poca 17 com AUC de validaÃ§Ã£o de 85.07\%.

A Tabela~\ref{tab:cross_dataset} apresenta os resultados da avaliaÃ§Ã£o 
cross-dataset, evidenciando F1-Score consistente de aproximadamente 92.9\% 
em ambos os datasets testados.
```

---

## âœ… CHECKLIST FINAL PARA O TCC

### Antes de Submeter

- [ ] Copiar todas as 4 figuras de `outputs/figures/` para pasta de imagens do LaTeX
- [ ] Verificar resoluÃ§Ã£o das figuras (devem estar em 300 DPI)
- [ ] Inserir legendas completas e descritivas em cada figura
- [ ] Adicionar referÃªncias no texto para todas as figuras e tabelas
- [ ] Verificar consistÃªncia dos valores citados no texto com as tabelas
- [ ] Incluir `table_metrics.csv` como tabela no apÃªndice (opcional)
- [ ] Citar o relatÃ³rio tÃ©cnico (`run_report.md`) como documentaÃ§Ã£o adicional
- [ ] Validar que todas as mÃ©tricas citadas estÃ£o corretas

### Arquivos a Incluir

**ObrigatÃ³rios** (no documento):
- [x] 4 figuras PNG em alta resoluÃ§Ã£o
- [x] 2-3 tabelas com mÃ©tricas principais
- [x] Textos descritivos adaptados

**Opcionais** (apÃªndice ou material suplementar):
- [ ] `table_metrics.csv` - Tabela completa de mÃ©tricas
- [ ] `metrics_train.csv` - HistÃ³rico completo de treino
- [ ] `metrics_cross.csv` - Detalhes cross-dataset
- [ ] `run_report.md` - RelatÃ³rio tÃ©cnico completo
- [ ] Exemplos de heatmaps individuais do Grad-CAM

---

## ðŸ“ž VALIDAÃ‡ÃƒO FINAL

**Para garantir que tudo estÃ¡ correto, execute**:

```bash
python validate_outputs.py
```

**SaÃ­da esperada**: âœ… VALIDAÃ‡ÃƒO COMPLETA: TODOS OS OUTPUTS ESTÃƒO OK!

---

## ðŸŽ‰ PRONTO!

Todos os outputs foram:
- âœ… **Limpos** (removidos dados antigos/inconsistentes)
- âœ… **Regenerados** (a partir dos dados brutos validados)
- âœ… **Validados** (consistÃªncia verificada)
- âœ… **Documentados** (guias e textos prontos)

**Agora vocÃª pode focar em**:
1. Adaptar os textos ao seu estilo de escrita
2. Inserir as figuras e tabelas no LaTeX
3. Revisar as referÃªncias e citaÃ§Ãµes
4. Finalizar o documento do TCC

**Boa sorte com o TCC! ðŸŽ“**

---

**Criado em**: 1 de novembro de 2025  
**Validado**: âœ… Todos os 11 arquivos obrigatÃ³rios presentes e corretos
