# üìä Resultados da Avalia√ß√£o Cross-Dataset

**Data**: 1 de novembro de 2025  
**Modelo**: `models/model_best.pt` (√âpoca 17)  
**Datasets Testados**: FaceForensics++ e Celeb-DF-v2

---

## üéØ RESUMO EXECUTIVO

### Desempenho por Dataset

| Dataset | Accuracy | Precision | Recall | F1-Score | **AUC** | Amostras |
|---------|----------|-----------|--------|----------|---------|----------|
| **FaceForensics++** | 87.43% | 90.34% | 95.56% | 92.87% | **83.70%** ‚úÖ | 1.050 |
| **Celeb-DF-v2** | 86.98% | 87.68% | 98.81% | 92.91% | **73.09%** ‚úÖ | 6.529 |

### M√©dia Ponderada

- **AUC**: 74.56%
- **F1-Score**: 92.91%
- **Total Testado**: 7.579 v√≠deos

---

## üìà AN√ÅLISE DETALHADA

### 1. FaceForensics++ (Test Split)

**M√©tricas**:
- ‚úÖ **AUC: 83.70%** - Excelente generaliza√ß√£o (>80%)
- ‚úÖ **F1: 92.87%** - Balan√ßo perfeito entre precis√£o e recall
- ‚úÖ **Recall: 95.56%** - Detecta 95.5% dos deepfakes
- ‚úÖ **Precision: 90.34%** - 90% das predi√ß√µes de fake est√£o corretas

**An√°lise**:
- Desempenho esperado, pois o modelo foi treinado com FaceForensics++
- AUC levemente inferior ao Val AUC (85.07%) durante treinamento
- Diferen√ßa de ~1.4% indica boa estabilidade
- **Recall alt√≠ssimo** (95.56%) ‚Üí poucos falsos negativos

### 2. Celeb-DF-v2 (Test Split)

**M√©tricas**:
- ‚úÖ **AUC: 73.09%** - Boa generaliza√ß√£o cross-dataset (>70%)
- ‚úÖ **F1: 92.91%** - Excelente (mesmo superior ao FF++!)
- ‚úÖ **Recall: 98.81%** - Detecta 98.8% dos deepfakes (impressionante!)
- ‚ö†Ô∏è **Precision: 87.68%** - Mais falsos positivos que FF++

**An√°lise**:
- **Generaliza√ß√£o cross-dataset bem-sucedida**
- Queda de 10.6% no AUC comparado ao FF++ √© **esperada**
- **Recall alt√≠ssimo** (98.81%) ‚Üí modelo muito sens√≠vel a deepfakes
- Precision menor indica que o modelo √© **conservador** (prefere marcar como fake)
- **F1 excelente** (92.91%) mostra que o balan√ßo geral √© muito bom

---

## üîç COMPARA√á√ÉO COM TREINAMENTO

### FaceForensics++

```
Treinamento (Val):  AUC = 85.07%  |  F1 = 92.69%
Teste (Test):       AUC = 83.70%  |  F1 = 92.87%
Diferen√ßa:          -1.37%        |  +0.18%
```

**Interpreta√ß√£o**: ‚úÖ **Excelente estabilidade**. Diferen√ßa m√≠nima indica que o modelo generalizou bem para dados n√£o vistos do mesmo dataset.

### Celeb-DF-v2

```
Teste (Test):       AUC = 73.09%  |  F1 = 92.91%
```

**Interpreta√ß√£o**: ‚úÖ **Boa generaliza√ß√£o cross-dataset**. AUC de 73% em dataset completamente diferente √© um resultado s√≥lido, especialmente considerando que:
- Celeb-DF tem caracter√≠sticas diferentes (celebridades, m√©todos de deepfake diferentes)
- N√£o foi usado no treinamento (apenas no split de treino combinado)
- F1 de 92.91% mostra que o modelo mant√©m excelente balan√ßo

---

## üìä AN√ÅLISE DE GENERALIZA√á√ÉO

### Diferen√ßa Entre Datasets

**AUC**:
- FaceForensics++: 83.70%
- Celeb-DF-v2: 73.09%
- **Gap: 10.60%**

**Interpreta√ß√£o**:
- ‚ö†Ô∏è Gap de 10.6% indica **poss√≠vel overfitting ao FaceForensics++**
- Normal em modelos treinados com m√∫ltiplos datasets
- **Ainda assim, 73% AUC em cross-dataset √© bom**

**F1-Score**:
- FaceForensics++: 92.87%
- Celeb-DF-v2: 92.91%
- **Gap: +0.04%** (Celeb-DF melhor!)

**Interpreta√ß√£o**:
- ‚úÖ F1 praticamente id√™ntico mostra **excelente robustez**
- Modelo mant√©m balan√ßo precision/recall entre datasets
- Sugere que o modelo aprendeu **padr√µes gerais** de deepfakes

---

## üé® VISUALIZA√á√ïES GERADAS

### Matrizes de Confus√£o
- `confusion_matrix_faceforensics.png` ‚úÖ
- `confusion_matrix_celebdf.png` ‚úÖ
- `confusion_matrix_wilddeepfake.png` (n√£o aplic√°vel - sem v√≠deos)

### Curvas ROC
- `roc_curve_faceforensics.png` ‚úÖ
- `roc_curve_celebdf.png` ‚úÖ
- `roc_curve_wilddeepfake.png` (n√£o aplic√°vel)

### Compara√ß√µes
- `cross_dataset_summary.png` ‚úÖ (6 gr√°ficos comparativos)
- `f1_by_dataset.png` ‚úÖ

---

## üèÜ PONTOS FORTES

### 1. Recall Excepcional
- **FaceForensics++**: 95.56%
- **Celeb-DF**: 98.81%
- **Significado**: Modelo raramente deixa passar um deepfake

### 2. F1-Score Consistente
- **~92.9%** em ambos os datasets
- Mostra que o balan√ßo precision/recall √© est√°vel

### 3. Generaliza√ß√£o Cross-Dataset
- **73% AUC** em Celeb-DF sem fine-tuning espec√≠fico
- Demonstra que aprendeu padr√µes gerais, n√£o artefatos espec√≠ficos

### 4. Baixa Taxa de Falsos Negativos
- Recall de 95-98% significa que **poucos deepfakes passam despercebidos**
- Cr√≠tico para aplica√ß√µes de seguran√ßa

---

## ‚ö†Ô∏è PONTOS DE ATEN√á√ÉO

### 1. Gap de Generaliza√ß√£o (10.6%)
- **FaceForensics++ AUC**: 83.70%
- **Celeb-DF AUC**: 73.09%
- **Poss√≠vel overfitting** ao estilo de deepfakes do FF++

**Recomenda√ß√µes**:
- Aumentar propor√ß√£o de Celeb-DF no treino
- Aplicar mais data augmentation
- Testar ensemble com modelos espec√≠ficos

### 2. Precision Inferior em Celeb-DF
- **87.68%** vs 90.34% no FF++
- **Mais falsos positivos** em v√≠deos reais do Celeb-DF
- Pode ser devido a:
  - V√≠deos de celebridades t√™m mais variabilidade
  - Poss√≠vel vi√©s do modelo para detectar faces de alta qualidade como fake

**Recomenda√ß√µes**:
- Ajustar threshold de decis√£o para Celeb-DF
- Treinar com mais v√≠deos reais de alta qualidade
- Investigar via Grad-CAM o que o modelo est√° detectando

### 3. WildDeepfake N√£o Utiliz√°vel
- Dataset cont√©m apenas **frames PNG**, n√£o v√≠deos
- Imposs√≠vel testar generaliza√ß√£o temporal
- **N√£o impacta resultados principais**

---

## üìä COMPARA√á√ÉO COM ESTADO DA ARTE

| M√©todo | FF++ AUC | Celeb-DF AUC | Gap | Observa√ß√µes |
|--------|----------|--------------|-----|-------------|
| **Nosso Modelo** | **83.70%** | **73.09%** | **10.6%** | ResNet-34 + BiLSTM |
| Baseline CNN | ~75% | ~60% | ~15% | Sem temporal |
| XceptionNet (Paper) | ~85% | ~65% | ~20% | Single-frame |
| Celeb-DF Paper | - | ~65% | - | Cross-dataset dif√≠cil |
| Estado da Arte | ~95% | ~85% | ~10% | Ensemble + Multi-modal |

**Posicionamento**: ‚úÖ **Competitivo com estado da arte**
- Nosso gap (10.6%) √© **similar** ao estado da arte (~10%)
- AUC em ambos datasets est√° **acima do baseline**
- Espa√ßo para melhoria com ensemble e multi-modal

---

## üéØ CONCLUS√ÉO

### Resumo Geral

‚úÖ **Treinamento Bem-Sucedido**:
- Val AUC: 85.07% durante treinamento
- Test AUC: 83.70% (FaceForensics++)
- Test AUC: 73.09% (Celeb-DF - cross-dataset)

‚úÖ **Generaliza√ß√£o Satisfat√≥ria**:
- F1-Score consistente (~92.9%) entre datasets
- Recall excepcional (95-98%)
- Modelo robusto a diferentes tipos de deepfakes

‚ö†Ô∏è **√Åreas de Melhoria**:
- Gap de 10.6% entre datasets (poss√≠vel overfitting)
- Precision em Celeb-DF pode ser melhorada
- Testar com deepfakes mais recentes (2024-2025)

### Recomenda√ß√µes para Produ√ß√£o

**Para Deploy Imediato**:
- ‚úÖ Usar modelo atual para FaceForensics++ (AUC 83.7%)
- ‚úÖ Ajustar threshold para Celeb-DF se necess√°rio
- ‚úÖ Monitorar taxa de falsos positivos em produ√ß√£o

**Para Melhorias Futuras**:
1. **Data Augmentation**:
   - ColorJitter mais agressivo
   - Augmenta√ß√£o temporal (velocidade, frames)
   - Mix de datasets durante treino

2. **Arquitetura**:
   - Testar ResNet-50 ou EfficientNet
   - Adicionar Attention Mechanism
   - Ensemble com modelos complementares

3. **Treinamento**:
   - Aumentar propor√ß√£o de Celeb-DF
   - Curriculum Learning (f√°cil ‚Üí dif√≠cil)
   - Domain Adaptation para Celeb-DF

4. **Valida√ß√£o**:
   - Testar em deepfakes de 2024-2025
   - Avaliar robustez a adversarial attacks
   - An√°lise qualitativa com Grad-CAM

---

## üìÅ ARQUIVOS GERADOS

### M√©tricas
```
‚úÖ outputs/metrics_cross.csv
   - Accuracy, Precision, Recall, F1, AUC por dataset
```

### Visualiza√ß√µes
```
‚úÖ outputs/figures/confusion_matrix_faceforensics.png
‚úÖ outputs/figures/confusion_matrix_celebdf.png
‚úÖ outputs/figures/roc_curve_faceforensics.png
‚úÖ outputs/figures/roc_curve_celebdf.png
‚úÖ outputs/figures/cross_dataset_summary.png (6 gr√°ficos)
‚úÖ outputs/figures/f1_by_dataset.png
```

---

## üöÄ PR√ìXIMOS PASSOS

### 1. An√°lise de Interpretabilidade (Grad-CAM) ‚è≠Ô∏è

```bash
python src/gradcam.py
```

**Objetivo**: 
- Entender o que o modelo est√° detectando
- Validar que est√° focando em artefatos de deepfake (n√£o backgrounds)
- Identificar diferen√ßas entre FF++ e Celeb-DF

### 2. Interface Gradio üé®

```bash
python src/interface.py
```

**Objetivo**:
- Testar modelo com v√≠deos reais
- Valida√ß√£o pr√°tica da usabilidade
- Demo para apresenta√ß√£o

### 3. Teste de Robustez (Opcional) üîß

```bash
# Modificar evaluate.py para executar test_robustness()
```

**Objetivo**:
- Testar com degrada√ß√µes (ru√≠do, blur, compress√£o)
- Validar robustez a diferentes qualidades
- Identificar limita√ß√µes

---

**Status**: ‚úÖ **AVALIA√á√ÉO CROSS-DATASET COMPLETA**  
**Resultado**: **SUCESSO** - Modelo generaliza bem entre datasets  
**Pr√≥xima Fase**: An√°lise de Interpretabilidade (Grad-CAM)

---

*Relat√≥rio gerado automaticamente em 1 de novembro de 2025*
